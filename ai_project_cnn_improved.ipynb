{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandeshbhatta495/AI_lab/blob/main/ai_project_cnn_improved.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CIFAR-10 Classification: Comparing Simple , AlexNet, and TinyVGG\n"
      ],
      "metadata": {
        "id": "YlW4WxlMF8mK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision"
      ],
      "metadata": {
        "id": "uDkEPSHtF8mP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDDrPtmLbQ3s",
        "outputId": "c20f09e2-cc20-4309-cf21-453d44bc5537"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Loading and Augmentation"
      ],
      "metadata": {
        "id": "0ktd2IV2F8mR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                         (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                         (0.5, 0.5, 0.5))\n",
        "])"
      ],
      "metadata": {
        "id": "pf_iLyTHF8mR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What Does Data Augmentation Do?\n",
        "\n",
        "Data augmentation generates new training samples from the original images by applying small transformations such as flipping, rotation, cropping, zooming, or shifting. Instead of feeding the model the exact same image every time, we show it slightly modified versions, which helps prevent the model from memorizing the training data and overfitting. By exposing the model to these variations, it learns general patterns rather than relying on exact pixel positions, becomes more robust to real-world variations, and improves overall generalization and test accuracy. In simple terms, data augmentation makes the dataset appear larger and more diverse, even though no new data was actually collected."
      ],
      "metadata": {
        "id": "meNa0fl1eHTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Dataset CIFAR-10\n",
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=train_transform\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=test_transform\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5iMZP1oF8mS",
        "outputId": "a246d161-f9c7-4be4-986c-454044a33ec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▎      | 57.4M/170M [02:45<07:53, 239kB/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Loaders\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,          # Shuffle training data each epoch to avoid order bias\n",
        "    num_workers=2          # Use 2 background threads to load data faster\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,         # No need to shuffle test data\n",
        "    num_workers=2\n",
        ")"
      ],
      "metadata": {
        "id": "kUWTlCSQdsEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# My Data info\n",
        "CLASS_NAMES = train_dataset.classes\n",
        "print('Classes:', CLASS_NAMES)\n",
        "print('\\nTraining samples:', len(train_dataset))\n",
        "print('\\nTest samples:', len(test_dataset))"
      ],
      "metadata": {
        "id": "2_5qA7tpdxg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def imshow(img_tensor):\n",
        "    img_tensor = img_tensor / 2 + 0.5\n",
        "    np_image = img_tensor.numpy()\n",
        "    plt.imshow(np.transpose(np_image, (1, 2, 0)))\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "data_iter = iter(train_loader)\n",
        "sample_images, sample_labels = next(data_iter)\n",
        "\n",
        "imshow(torchvision.utils.make_grid(sample_images[:8]))\n",
        "print(\"Labels:\", [CLASS_NAMES[label] for label in sample_labels[:8]])"
      ],
      "metadata": {
        "id": "_e-KdG3_F8mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. For Simple NN"
      ],
      "metadata": {
        "id": "-bTxwFHcF8mT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "\n",
        "        # Convert (batch, 3, 32, 32) → (batch, 3072)\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Fully Connected Block 1\n",
        "        self.fc1 = nn.Linear(32 * 32 * 3, 512)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "\n",
        "        # Fully Connected Block 2\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "\n",
        "        # Output layer (10 classes)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)                       # Flatten image\n",
        "        x = self.dropout1(self.relu1(self.fc1(x)))  # FC1 → ReLU → Dropout\n",
        "        x = self.dropout2(self.relu2(self.fc2(x)))  # FC2 → ReLU → Dropout\n",
        "        x = self.fc3(x)                           # Final logits\n",
        "        return x"
      ],
      "metadata": {
        "id": "6BKws4ZZF8mT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Neural Network, This model treats each image as a flat vector of pixels, here we use Droup out 0.3 mean 30 % of neuron is deselected at training time which prevents model from overfittings"
      ],
      "metadata": {
        "id": "W6t4tqAAgkgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Model 2  AlexNet\n",
        "class AlexNet_CIFAR10(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AlexNet_CIFAR10, self).__init__()\n",
        "\n",
        "        # Input: (batch, 3, 32, 32)\n",
        "        self.features = nn.Sequential(\n",
        "            # Fist layer\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),   # 32x32 → 16x16\n",
        "\n",
        "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),   # 16x16 → 8x8\n",
        "\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)    # 8x8 → 4x4\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256 * 4 * 4, 512),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)          # Extract spatial features\n",
        "        x = torch.flatten(x, 1)       # Flatten: (batch, 256, 4, 4) → (batch, 4096)\n",
        "        x = self.classifier(x)        # Map features to class logits\n",
        "        return x"
      ],
      "metadata": {
        "id": "q5Qj8mFZF8mU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 3: TinyVGG\n",
        "class TinyVGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TinyVGG, self).__init__()\n",
        "        # Input: (batch, 3, 32, 32)\n",
        "        self.features = nn.Sequential(\n",
        "            # Block 1\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),    # 32x32 → 16x16\n",
        "\n",
        "            # Block 2: increase depth (64 channels)\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)     # 16x16 → 8x8\n",
        "        )\n",
        "\n",
        "        # Classifier\n",
        "        # Input size: 64 channels × 8 × 8 = 4096 values\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(64 * 8 * 8, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)     # (batch, 64, 8, 8) → (batch, 4096)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "4Kk4rDOGF8mU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count Trainable Parameters\n",
        "def count_parameters(model):\n",
        "    \"\"\"Return total number of learnable parameters in the model.\"\"\"\n",
        "    total = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f'  Trainable parameters: {total:,}')\n",
        "    return total"
      ],
      "metadata": {
        "id": "Y_jppeslF8mV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def train_model(model, optimizer, scheduler, num_epochs=20, model_name='Model'):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()  # Loss function for multi-class classification\n",
        "\n",
        "    loss_history = []\n",
        "    accuracy_history = []\n",
        "\n",
        "    start_time = time.time()  # Measure total training time\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # One epoch = model sees the entire training dataset once\n",
        "        model.train()  # Enable training mode (Dropout, BatchNorm active)\n",
        "\n",
        "        running_loss = 0.0   # Total loss for this epoch\n",
        "        correct = 0          # Correct predictions counter\n",
        "        total = 0            # Total samples counter\n",
        "\n",
        "        for batch_images, batch_labels in train_loader:\n",
        "            # Each batch = small part of dataset for efficient training\n",
        "\n",
        "            batch_images = batch_images.to(device)\n",
        "            batch_labels = batch_labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()  # Clear old gradients\n",
        "\n",
        "            outputs = model(batch_images)  # Forward pass (get predictions)\n",
        "            loss = criterion(outputs, batch_labels)  # Compute error\n",
        "\n",
        "            loss.backward()  # Compute gradients (backpropagation)\n",
        "            optimizer.step()  # Update model weights\n",
        "\n",
        "            running_loss += loss.item()  # Add batch loss to epoch loss\n",
        "\n",
        "            _, predicted_classes = torch.max(outputs, 1)  # Get predicted class\n",
        "            total += batch_labels.size(0)\n",
        "            correct += (predicted_classes == batch_labels).sum().item()\n",
        "\n",
        "        # Average loss across all batches in this epoch\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "\n",
        "        # Accuracy = correct predictions / total samples\n",
        "        train_accuracy = 100.0 * correct / total\n",
        "\n",
        "        loss_history.append(avg_loss)\n",
        "        accuracy_history.append(train_accuracy)\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()  # Adjust learning rate if scheduler is used\n",
        "\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        # Print performance after each epoch\n",
        "        print(f'[{model_name}] Epoch {epoch+1:2d}/{num_epochs} | '\n",
        "              f'Loss: {avg_loss:.4f} | '\n",
        "              f'Train Acc: {train_accuracy:.2f}% | '\n",
        "              f'LR: {current_lr:.6f}')\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f'[{model_name}] Training finished in {total_time:.1f} seconds')\n",
        "\n",
        "    return loss_history, accuracy_history, total_time"
      ],
      "metadata": {
        "id": "UOu7gyfSF8mV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Function\n",
        "def evaluate_model(model, model_name='Model'):\n",
        "\n",
        "    model.eval()  # Set model to evaluation mode (Dropout & BatchNorm disabled)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Disable gradient calculation to save memory and speed up testing\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch_images, batch_labels in test_loader:\n",
        "            # Load data to same device (CPU/GPU)\n",
        "            batch_images = batch_images.to(device)\n",
        "            batch_labels = batch_labels.to(device)\n",
        "\n",
        "            outputs = model(batch_images)  # Forward pass only (no backprop)\n",
        "\n",
        "            # Get class with highest predicted score\n",
        "            _, predicted_classes = torch.max(outputs, dim=1)\n",
        "\n",
        "            total += batch_labels.size(0)\n",
        "            correct += (predicted_classes == batch_labels).sum().item()\n",
        "\n",
        "    # Accuracy = correct predictions / total samples\n",
        "    accuracy = 100.0 * correct / total\n",
        "\n",
        "    print(f'[{model_name}] Test Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "lAsDmJAfF8mW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Train All Three Models\n"
      ],
      "metadata": {
        "id": "P7mAGH4yF8mX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 1: SimpleNN with Adam"
      ],
      "metadata": {
        "id": "dIEcfjM1SoQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple NN\n",
        "print('Simple Neural Network')\n",
        "model_simplenn = SimpleNN().to(device)\n",
        "num_params_simplenn = count_parameters(model_simplenn)\n",
        "\n",
        "optimizer_adam_nn = optim.Adam(\n",
        "    model_simplenn.parameters(),\n",
        "    lr=0.001,\n",
        "    weight_decay=1e-4  # L2 regularisation\n",
        ")\n",
        "#lr shedule\n",
        "scheduler_nn = optim.lr_scheduler.StepLR(\n",
        "    optimizer_adam_nn,\n",
        "    step_size=5,\n",
        "    gamma=0.5\n",
        "    )\n",
        "\n",
        "# Train for 20 epochs\n",
        "loss_nn, acc_nn, time_nn = train_model(\n",
        "    model_simplenn, optimizer_adam_nn, scheduler_nn,\n",
        "    num_epochs=20, model_name='SimpleNN'\n",
        ")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_acc_nn = evaluate_model(model_simplenn, model_name='SimpleNN')"
      ],
      "metadata": {
        "id": "C5kzR1dmF8mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 2: AlexNet with SGD + Momentum"
      ],
      "metadata": {
        "id": "lsl3CHaMF8mX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 2 : AlexNet\n",
        "print(\"Ale\")\n",
        "model_alexnet = AlexNet_CIFAR10().to(device)\n",
        "num_params_alexnet = count_parameters(model_alexnet)\n",
        "\n",
        "optimizer_sgd_alexnet = optim.SGD(\n",
        "    model_alexnet.parameters(),\n",
        "    lr=0.01,\n",
        "    momentum=0.9,\n",
        "    weight_decay=5e-4  # L2 regularisation\n",
        ")\n",
        "# lr scheduler\n",
        "scheduler_alexnet = optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer_sgd_alexnet,\n",
        "    T_max=30,\n",
        "    eta_min=1e-4\n",
        "\n",
        "# Train for 30 epochs (CNNs need more time to converge)\n",
        "loss_alexnet, acc_alexnet, time_alexnet = train_model(\n",
        "    model_alexnet, optimizer_sgd_alexnet, scheduler_alexnet,\n",
        "    num_epochs=30, model_name='AlexNet'\n",
        ")\n",
        "\n",
        "test_acc_alexnet = evaluate_model(model_alexnet, model_name='AlexNet')"
      ],
      "metadata": {
        "id": "iJgNYxsDF8mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 3: TinyVGG (opti. SGD + Momentum)"
      ],
      "metadata": {
        "id": "5CyQJf-kF8mX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 3 : TinyVGG\n",
        "model_tinyvgg = TinyVGG().to(device)\n",
        "num_params_tinyvgg = count_parameters(model_tinyvgg)\n",
        "\n",
        "optimizer_sgd_vgg = optim.SGD(\n",
        "    model_tinyvgg.parameters(),\n",
        "    lr=0.05,\n",
        "    momentum=0.9,\n",
        "    weight_decay=5e-4\n",
        ")\n",
        "\n",
        "scheduler_tinyvgg = optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer_sgd_vgg,\n",
        "    T_max=30,\n",
        "    eta_min=1e-4\n",
        ")\n",
        "\n",
        "loss_tinyvgg, acc_tinyvgg, time_tinyvgg = train_model(\n",
        "    model_tinyvgg, optimizer_sgd_vgg, scheduler_tinyvgg,\n",
        "    num_epochs=30, model_name='TinyVGG'\n",
        ")\n",
        "\n",
        "test_acc_tinyvgg = evaluate_model(model_tinyvgg, model_name='TinyVGG')"
      ],
      "metadata": {
        "id": "KL9XQtj9F8mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Comparison and Visualisation"
      ],
      "metadata": {
        "id": "9IMgBdkeF8mY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Training Accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, len(acc_nn) + 1), acc_nn, label='SimpleNN (Adam)')\n",
        "plt.plot(range(1, len(acc_alexnet) + 1), acc_alexnet, label='AlexNet (SGD)')\n",
        "plt.plot(range(1, len(acc_tinyvgg) + 1), acc_tinyvgg, label='TinyVGG (SGD)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Training Accuracy')\n",
        "plt.title('Accuracy per Epoch')\n",
        "plt.legend()\n",
        "\n",
        "# Training Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, len(loss_nn) + 1), loss_nn, label='SimpleNN (Adam)')\n",
        "plt.plot(range(1, len(loss_alexnet) + 1), loss_alexnet, label='AlexNet (SGD)')\n",
        "plt.plot(range(1, len(loss_tinyvgg) + 1), loss_tinyvgg, label='TinyVGG (SGD)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss per Epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_curves.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sd-4OgkQF8mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nModel            Params     Train Time(s)   Test Acc (%)\")\n",
        "print(\"----------------------------------------------------------\")\n",
        "\n",
        "print(f\"SimpleNN   {num_params_simplenn:>12,}   {time_nn:>14.1f}   {test_acc_nn:>12.2f}\")\n",
        "print(f\"AlexNet    {num_params_alexnet:>12,}   {time_alexnet:>14.1f}   {test_acc_alexnet:>12.2f}\")\n",
        "print(f\"TinyVGG    {num_params_tinyvgg:>12,}   {time_tinyvgg:>14.1f}   {test_acc_tinyvgg:>12.2f}\")"
      ],
      "metadata": {
        "id": "y8rBkKX0F8mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Discussion and Conclusions\n",
        "\n",
        "### Architecture Comparison\n",
        "\n",
        "The results confirm the expected hierarchy of the three models. The **SimpleNN (MLP)** treats each image as a flat vector of 3,072 pixel values, which discards all spatial structure. Pixels that are neighbours in the image have no special relationship in the model, so it must learn everything from scratch using global connections. As a result, it converges slowly and plateaus at a relatively low test accuracy (~45–52%). Despite having roughly 1.6 million parameters, it uses them inefficiently because it cannot exploit translation invariance.\n",
        "\n",
        "**AlexNet** introduces convolutional layers, which share weights across spatial locations. Each filter scans the entire image and detects the same pattern regardless of where it appears — this is called *translation equivariance*. AlexNet stacks five convolutional layers with increasing depth (64 → 192 → 384 → 256 → 256 channels), allowing it to learn hierarchical features: edges in early layers, textures in the middle, and object parts in deeper layers. Dropout (p=0.5) in the fully connected layers prevents co-adaptation between neurons, significantly reducing overfitting. The result is a substantial accuracy improvement over the MLP.\n",
        "\n",
        "**TinyVGG** applies the VGGNet philosophy: use only small 3×3 kernels, but stack two in each block before pooling. Two consecutive 3×3 convolutions have the same receptive field as one 5×5 convolution, but with fewer parameters and an extra non-linearity between them — this increases representational power. TinyVGG achieves competitive accuracy with fewer parameters than AlexNet, demonstrating that network depth and efficient design often matter more than raw parameter count.\n",
        "\n",
        "### Optimizer and Scheduler Choices\n",
        "\n",
        "**Adam** was chosen for the SimpleNN because its adaptive per-parameter learning rates help navigate flat loss landscapes common in fully connected networks. For a model that flattens spatial information, the loss surface is more complex and Adam's adaptivity provides a clear benefit over vanilla SGD.\n",
        "\n",
        "**SGD with momentum (0.9)** was chosen for both CNNs. Momentum accumulates a velocity vector in directions of persistent gradient, helping the optimiser pass through narrow valleys and small local minima. Empirically, SGD with momentum generalises better than Adam for convolutional architectures, a finding reported in multiple papers including the original ResNet work.\n",
        "\n",
        "**CosineAnnealingLR** was applied to both CNNs. It reduces the learning rate following a cosine curve, allowing aggressive learning early and precise weight adjustment later. This consistently outperforms fixed learning rates and even StepLR for CNNs trained on CIFAR-10.\n",
        "\n",
        "### AlexNet Improvements Over Earlier CNNs\n",
        "\n",
        "AlexNet introduced several innovations that became standard in deep learning: (1) ReLU activations instead of sigmoid or tanh, which train ~6× faster due to non-saturation; (2) Dropout regularisation to prevent overfitting in fully connected layers; (3) training on GPU, enabling much deeper networks; (4) Local Response Normalisation (not used here, as Batch Normalization is now preferred). These contributions made deep CNNs practically trainable for the first time on large datasets.\n",
        "\n",
        "### Conclusions\n",
        "\n",
        "Convolutional architectures decisively outperform flat MLPs on image classification tasks. The CNN models learn meaningful spatial features through weight sharing, dramatically improving both accuracy and parameter efficiency. Between the two CNNs, TinyVGG demonstrates that thoughtful architecture design (small stacked kernels, efficient depth) can match or exceed more complex models with fewer parameters. Future work could explore Batch Normalization, residual connections (ResNet), or transfer learning from pretrained models to push accuracy beyond 90% on CIFAR-10."
      ],
      "metadata": {
        "id": "3XbwmLaZF8mY"
      }
    }
  ]
}