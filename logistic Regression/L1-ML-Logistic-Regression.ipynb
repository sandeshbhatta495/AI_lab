{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14820f96",
   "metadata": {},
   "source": [
    "# Lab 1: Machine Learning Supervised Binary Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779150eb",
   "metadata": {},
   "source": [
    "References: \n",
    "- [The Machine Learning Simplified: A Gentle Introduction to Supervised Learning](https://www.amazon.com/dp/B0B216KMM4/qid=1653304321)\n",
    "- [Machine Learning for Neuroscience Notebook](https://github.com/PBarnaghi/ML4NS/blob/main/00-%20Tutorials/Machine%20Learning%20for%20Beginners%20Tutorial%20and%20Assessment/Machine%20Learning%20for%20Beginners%20(run).ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3ed1b0",
   "metadata": {},
   "source": [
    "# Artificial Intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0091f750",
   "metadata": {},
   "source": [
    "![AI-ML-DL-Data-Science](https://miro.medium.com/v2/resize:fit:1358/1*UQiAwDtQHP_MunDx5QfnHw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be00fa4a",
   "metadata": {},
   "source": [
    "**Artificial Intelligence** (AI): \n",
    "- Anything a machine does that looks like a human task — could be simple code, rules, or a smart system.\n",
    "- Example: A program that sends a reminder email when a due date passes. It's \"AI\" if we call that behavior intelligent.\n",
    "\n",
    "**Machine Learning** (ML): \n",
    "- The system learns from data and finds patterns instead of being fully hard-coded.\n",
    "- Example: A model that looks at past invoices and learns which ones get paid late, then predicts future late payments.\n",
    "\n",
    "**Deep Learning** (DL): \n",
    "- A kind of ML that uses deep neural networks (many layers) and often works directly with raw data (images, audio, text).\n",
    "- Example: A CNN that learns from raw photos to decide if there's a cat — it figures out edges, shapes, and features automatically.\n",
    "\n",
    "**Data Science**: \n",
    "- The broader process around data — collecting, cleaning, exploring, visualizing, and using ML/DL to answer questions and make decisions.\n",
    "- Example: Inspecting sales data, cleaning it, plotting trends, building a model to forecast demand, and reporting the result.\n",
    "\n",
    "In summary, \n",
    "- AI = any machine behavior we call “intelligent.”\n",
    "- ML = AI that learns from data.\n",
    "- DL = ML using deep neural networks.\n",
    "- Data Science = turning data into insight (includes ML/DL)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396714a0",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "- The basic idea of machine learning, or ML, is to learn to do a certain task from data.\n",
    "- At a high-level it can be understood as appropriately recognizing and extracting patterns from the data.\n",
    "- According to [Wikipedia](https://en.wikipedia.org/wiki/Machine_learning): \n",
    "    - Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms \n",
    "        - that can **learn from data** and \n",
    "        - **generalize** to unseen data, and \n",
    "        - thus perform tasks **without** explicit **instructions**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8862892",
   "metadata": {},
   "source": [
    "**Machine Learning**\n",
    "- **Supervised Learning (Labelled Data) (this notebook)**\n",
    "    - Regression\n",
    "    - Classification\n",
    "- Unsupervised Learning (Unlabelled Data)\n",
    "- Reinforced Learning (not covered in the syllabus)\n",
    "\n",
    "**Deep Learning**\n",
    "- Multilayer Neural Network (MLP) (Multilayer Perceptron)\n",
    "- Convolutional NN\n",
    "- Sequence to Sequence Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e6c099",
   "metadata": {},
   "source": [
    "After the course you will be able to understand: \n",
    "- What kind of problems supervised (and unsupervised) ML can be used to solve;\n",
    "- How a typical supervised learning algorithm works;\n",
    "- The concepts of overfitting and underfitting;\n",
    "- Detailed pipeline for a full ML system;\n",
    "- How to quantitatively evaluate any model;\n",
    "- The inner workings of the gradient descent algorithm;\n",
    "- How we can use basis expansion to increase model’s complexity;\n",
    "- Why we need regularization and when it is helpful;\n",
    "- What types of errors any model consists of, and how to decrease/minimize them;\n",
    "- How to mathematically decompose bias and variance errors from a cost function;\n",
    "- Three main feature selection families;\n",
    "- Different feature selection procedures and philosophies;\n",
    "- What procedures in data preparation exist; (Data Cleaning, Feature Transformation, Feature Engineering, Class Label Imbalance)\n",
    "- Why we need them and how they are performed\n",
    "\n",
    "So, a question:\n",
    "<!-- <p style=\"margin-bottom: 200px;\"> <b>Q. What are the problems where we can utilize ML?</b></p> -->\n",
    "**Q. What are the problems where we can utilize ML?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f8001c",
   "metadata": {},
   "source": [
    "![Source: 2020 Machine Learning Roadmap Video](./resources/images/comment_2020_AI_roadmap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00daa852",
   "metadata": {},
   "source": [
    "Rule #1 of [Google's Machine Learning Handbook](https://developers.google.com/machine-learning/guides/rules-of-ml/)\n",
    "    - **If you can build a simple rule-based system that doesn't require ML, do that.**\n",
    "\n",
    "<!-- <p style=\"margin-bottom: 200px;\"> <b>If you can build a simple rule-based system that doesn't require ML, do that. </b></p> -->\n",
    "\n",
    "But, if you cannot, then you might possibly require to **use ML**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2c9e59",
   "metadata": {},
   "source": [
    "Say, your ML models (emphasis on models) cannot properly learn the pattern in the data then use DL. \n",
    "\n",
    "`Note`: As you model becomes **deeper**, the **interpretability** will be decreased. So as,  \n",
    "\n",
    "depth $\\uparrow$,  interpretablity $\\downarrow$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9f0381",
   "metadata": {},
   "source": [
    "## 1. Supervised ML Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41ad37f",
   "metadata": {},
   "source": [
    "![Source: 2020 Machine Learning Roadmap Video](./resources/images/ml_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db397c5",
   "metadata": {},
   "source": [
    "Question: Here, if this is the ML Pipeline, then what will the pipeline for DL will be? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d572d30f",
   "metadata": {},
   "source": [
    "### **1.1. Data Extraction**\n",
    "\n",
    "- Labels are usually obtained in one of two ways: \n",
    "    - Either by past observations (such as what price a house actually sold for) or \n",
    "    - By subjective evaluations of a human being (such as determining if an email is spam or not).\n",
    "\n",
    "`Note` that the supervised ML algorithm is not independently smart (as some sci-fi depictions of AI suggest) but instead is only learning to mimic\n",
    "the labels you gave it.\n",
    "\n",
    "- Garbage in, Garbage out. \n",
    "\n",
    "\n",
    "#### **1.1.0. Data Retreival & Collection**\n",
    "\n",
    "- Data retrieval and collection is the first step in any ML pipeline, involving gathering relevant raw data from diverse sources such as databases, APIs, sensors, and third-party providers. \n",
    "- Good collection practices ensure the data is representative, timely, and accompanied by metadata that supports reproducibility and traceability. \n",
    "- Attention to data quality, privacy, and legal constraints during collection helps prevent bias and compliance issues later in the pipeline. \n",
    "- Well-documented and structured retrieval workflows reduce downstream cleaning effort and improve model reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f07c64a",
   "metadata": {},
   "source": [
    "If available collect the data from reputable sources.\n",
    "\n",
    "If reputable sources are not available, then create one (i.e. yourself).\n",
    "\n",
    "For our example we will use a dataset from the `sklearn` library.\n",
    "\n",
    "But before that let's import some dependencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ca14cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23621b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import datasets\n",
    "# data = datasets.load_diabetes(as_frame=True) # this loads the dataset as a dictionary\n",
    "# features = data.data # this derives features as a dataframe (10 features by 442 instances)\n",
    "# labels = data.target # this derives a continuous-valued attribute as a dataframe (442 instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e5e75ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_breast_cancer(as_frame=True) # this loads the dataset as a dictionary\n",
    "features = data.data # this derives features as a dataframe (30 features by 569 instances)\n",
    "labels = data.target # this derives labels as a dataframe (569 instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535ea922",
   "metadata": {},
   "source": [
    "`features` are also called `inputs`. And, `labels` are also called `targets`. \n",
    "\n",
    "The breast cancer dataset is a classic and very easy binary classification dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf804d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (569, 30)\n",
      "Labels shape: (569,)\n"
     ]
    }
   ],
   "source": [
    "# check the shape of the dataset\n",
    "print(\"Features shape:\", features.shape)\n",
    "print(\"Labels shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "794ce4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features type: <class 'pandas.core.frame.DataFrame'>\n",
      "Labels type: <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# check the type of data\n",
    "\n",
    "print(\"Features type:\", type(features))\n",
    "print(\"Labels type:\", type(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd1ad8b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's  understand the data\n",
    "features.head(5) # first 5 rows of the features dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47e4964a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "564    0\n",
       "565    0\n",
       "566    0\n",
       "567    0\n",
       "568    1\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.tail(5) # last 5 rows of the labels dataframe/series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9e5f5a",
   "metadata": {},
   "source": [
    "Note: The labels are binary, with 0 indicating a negative diagnosis and 1 indicating a positive diagnosis of breast cancer, respectively.\n",
    "\n",
    "Here, this means this is a classification task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d721323a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>...</td>\n",
       "      <td>16.269190</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>...</td>\n",
       "      <td>4.833242</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>...</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>...</td>\n",
       "      <td>13.010000</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>...</td>\n",
       "      <td>14.970000</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.066120</td>\n",
       "      <td>...</td>\n",
       "      <td>18.790000</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.097440</td>\n",
       "      <td>...</td>\n",
       "      <td>36.040000</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean radius  mean texture  mean perimeter    mean area  \\\n",
       "count   569.000000    569.000000      569.000000   569.000000   \n",
       "mean     14.127292     19.289649       91.969033   654.889104   \n",
       "std       3.524049      4.301036       24.298981   351.914129   \n",
       "min       6.981000      9.710000       43.790000   143.500000   \n",
       "25%      11.700000     16.170000       75.170000   420.300000   \n",
       "50%      13.370000     18.840000       86.240000   551.100000   \n",
       "75%      15.780000     21.800000      104.100000   782.700000   \n",
       "max      28.110000     39.280000      188.500000  2501.000000   \n",
       "\n",
       "       mean smoothness  mean compactness  mean concavity  mean concave points  \\\n",
       "count       569.000000        569.000000      569.000000           569.000000   \n",
       "mean          0.096360          0.104341        0.088799             0.048919   \n",
       "std           0.014064          0.052813        0.079720             0.038803   \n",
       "min           0.052630          0.019380        0.000000             0.000000   \n",
       "25%           0.086370          0.064920        0.029560             0.020310   \n",
       "50%           0.095870          0.092630        0.061540             0.033500   \n",
       "75%           0.105300          0.130400        0.130700             0.074000   \n",
       "max           0.163400          0.345400        0.426800             0.201200   \n",
       "\n",
       "       mean symmetry  mean fractal dimension  ...  worst radius  \\\n",
       "count     569.000000              569.000000  ...    569.000000   \n",
       "mean        0.181162                0.062798  ...     16.269190   \n",
       "std         0.027414                0.007060  ...      4.833242   \n",
       "min         0.106000                0.049960  ...      7.930000   \n",
       "25%         0.161900                0.057700  ...     13.010000   \n",
       "50%         0.179200                0.061540  ...     14.970000   \n",
       "75%         0.195700                0.066120  ...     18.790000   \n",
       "max         0.304000                0.097440  ...     36.040000   \n",
       "\n",
       "       worst texture  worst perimeter   worst area  worst smoothness  \\\n",
       "count     569.000000       569.000000   569.000000        569.000000   \n",
       "mean       25.677223       107.261213   880.583128          0.132369   \n",
       "std         6.146258        33.602542   569.356993          0.022832   \n",
       "min        12.020000        50.410000   185.200000          0.071170   \n",
       "25%        21.080000        84.110000   515.300000          0.116600   \n",
       "50%        25.410000        97.660000   686.500000          0.131300   \n",
       "75%        29.720000       125.400000  1084.000000          0.146000   \n",
       "max        49.540000       251.200000  4254.000000          0.222600   \n",
       "\n",
       "       worst compactness  worst concavity  worst concave points  \\\n",
       "count         569.000000       569.000000            569.000000   \n",
       "mean            0.254265         0.272188              0.114606   \n",
       "std             0.157336         0.208624              0.065732   \n",
       "min             0.027290         0.000000              0.000000   \n",
       "25%             0.147200         0.114500              0.064930   \n",
       "50%             0.211900         0.226700              0.099930   \n",
       "75%             0.339100         0.382900              0.161400   \n",
       "max             1.058000         1.252000              0.291000   \n",
       "\n",
       "       worst symmetry  worst fractal dimension  \n",
       "count      569.000000               569.000000  \n",
       "mean         0.290076                 0.083946  \n",
       "std          0.061867                 0.018061  \n",
       "min          0.156500                 0.055040  \n",
       "25%          0.250400                 0.071460  \n",
       "50%          0.282200                 0.080040  \n",
       "75%          0.317900                 0.092080  \n",
       "max          0.663800                 0.207500  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d022ecdc",
   "metadata": {},
   "source": [
    "Common Statistical Outputs:\n",
    "\n",
    "- `count`: Number of non-NaN values.\n",
    "- `mean`: The average of the values.\n",
    "- `std`: Standard deviation of the values.\n",
    "- `min`: The smallest value.\n",
    "- `25%`: The 25th percentile (first quartile); means that 25% of the values in your data are below this number.\n",
    "- `50%`: The median (second quartile).\n",
    "- `75%`: The 75th percentile (third quartile).\n",
    "- `max`: The largest value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17362fa",
   "metadata": {},
   "source": [
    "Here, we are going to focus on two features (mean area and mean smoothness), so the first thing we'll do is extract this data.\n",
    "\n",
    "`Note`: This is done for ease of understanding. To reduce overwhelming the students. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a6195ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean area  mean smoothness\n",
       "0       1001.0          0.11840\n",
       "1       1326.0          0.08474\n",
       "2       1203.0          0.10960\n",
       "3        386.1          0.14250\n",
       "4       1297.0          0.10030\n",
       "..         ...              ...\n",
       "564     1479.0          0.11100\n",
       "565     1261.0          0.09780\n",
       "566      858.1          0.08455\n",
       "567     1265.0          0.11780\n",
       "568      181.0          0.05263\n",
       "\n",
       "[569 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "area_and_smoothness = features[['mean area', 'mean smoothness']]\n",
    "area_and_smoothness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86094181",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e0a60d",
   "metadata": {},
   "source": [
    "For this example, we are going to use logistic regression which, despite its name, is a ;`linear classification model`.\n",
    "\n",
    "To use the Logistic Regression classifier, we must first import it using the following line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "42a2518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99df2d47",
   "metadata": {},
   "source": [
    "### **2. Data Preparation**\n",
    "1. Data Cleaning\n",
    "2. Feature Design\n",
    "\n",
    "\n",
    "`Assumption`: Data is an ideal state. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921ebfee",
   "metadata": {},
   "source": [
    "#### **2.1. Data Cleaning**\n",
    "\n",
    "Practical datasets often have \n",
    "- missing values, \n",
    "- improperly scaled measurements, \n",
    "- outlier data points, or \n",
    "- non-numeric structured data (like strings) \n",
    "<!-- - erroneous or  -->\n",
    "that cannot be directly fed into the ML algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bf72734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking missing values\n",
    "# well we do not have to code any additional lines because in the .describe() method above, we can see that count for all features is 569 which means there are no missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa36913c",
   "metadata": {},
   "source": [
    "Feature scaling is one of the most critical pre-processing steps in machine learning, with the most common techniques being standardization and normalization.\n",
    "\n",
    "Machine learning algorithms that calculate distance or assume normality are sensitive to relative scales of features, meaning that if the data is not scaled, features with a higher value range start dominating the model's decision-making process. Feature scaling is therefore needed to bring features with different ranges into comparable ranges.\n",
    "\n",
    "Feature scaling also allows for much faster model convergence.\n",
    "\n",
    "In this scenario, we are going to use the sci-kit learn StandardScaler to standardize our data. Another commonly used scaler is the MinMax scaler. The MinMax scaler normalizes data. \n",
    "\n",
    "**Standardization (z-score)**\n",
    "- Shift the data so its average is 0 and scale it so a “typical” spread is 1. Useful when features have very different units (e.g., cm vs kg) so they contribute comparably to algorithms that use distances or assume roughly normal inputs.\n",
    "- Formula: z = (x − μ) / σ\n",
    "    - μ = mean of the feature, σ = standard deviation of the feature\n",
    "\n",
    "**Normalization (min–max)**\n",
    "- Squeeze the data into a fixed range (commonly 0 to 1). Keeps the shape of the distribution but rescales extremes to the chosen bounds — useful for neural nets or when you need values in a known interval.\n",
    "\n",
    "1. **Formula (to [0, 1]):**\n",
    "This formula scales the values of \\( x \\) to a range between 0 and 1.\n",
    "\n",
    "$$\n",
    "x' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $x$ is the original value,\n",
    "- $min(x)$ is the minimum value in the dataset,\n",
    "- $\\max(x)$ is the maximum value in the dataset,\n",
    "- $x'$ is the normalized value between 0 and 1.\n",
    "\n",
    "2. **Variant (to [−1, 1]):**\n",
    "This variant scales the values of \\( x \\) to a range between -1 and 1.\n",
    "\n",
    "$$\n",
    "x' = 2 \\times \\frac{x - \\min(x)}{\\max(x) - \\min(x)} - 1\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **x'** is the normalized value between -1 and 1.\n",
    "\n",
    "Notes\n",
    "- Standardization centers and scales; normalization rescales to a bounded interval.\n",
    "- Min–max is sensitive to outliers; standardization is less affected but still influenced by extreme values. Use robust scalers (median/IQR) if outliers are a concern.\n",
    "- In scikit‑learn: StandardScaler and MinMaxScaler implement these transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47ece747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f94ce5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.984375</td>\n",
       "      <td>1.568466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.908708</td>\n",
       "      <td>-0.826962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.558884</td>\n",
       "      <td>0.942210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.764464</td>\n",
       "      <td>3.283553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.826229</td>\n",
       "      <td>0.280372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean area  mean smoothness\n",
       "0   0.984375         1.568466\n",
       "1   1.908708        -0.826962\n",
       "2   1.558884         0.942210\n",
       "3  -0.764464         3.283553\n",
       "4   1.826229         0.280372"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next, let's implement our standardization.\n",
    "\n",
    "standardized_data = pd.DataFrame(scaler.fit_transform(area_and_smoothness),\n",
    "                   columns=['mean area','mean smoothness'])\n",
    "standardized_data.head(5) # first 5 rows of the standardized data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232920ec",
   "metadata": {},
   "source": [
    "For the rest, you need to write code to check them. Do ChatGPT, Grok, Gemini, or ask your Grandmother. You can do anything. But think of ways where, for the same , how would you find out the `outliers` and `non-numeric structured data` upper requirement. \n",
    "\n",
    "Other techniques similar to data cleaning (which get the data into an acceptable format to perform\n",
    "ML), are: \n",
    "- to convert numerical values into categories (called `feature binning`);\n",
    "- to convert from categorical values into numerical (called `feature encoding`); to scale feature measurements to a similar range\n",
    "\n",
    "This will be taught later, or material will be provided. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acca85d5",
   "metadata": {},
   "source": [
    "#### **2.2. Feature Design**\n",
    "\n",
    "**Feature Transformation**\n",
    "- What: Convert raw values into machine‑friendly numeric form (scaling, encoding, log transform).\n",
    "- When: You have categorical text, widely different numeric scales, or heavy skew.\n",
    "- Example: \"yes\"/\"no\" → 1/0; apply StandardScaler to features before training an SVM.\n",
    "- Tip: Encode before scaling; fit transformers on train set only.\n",
    "\n",
    "**Feature Engineering**\n",
    "- What: Create new features that capture useful relationships from raw features.\n",
    "- When: Domain knowledge or intuition suggests combinations/ratios/patterns might matter.\n",
    "- Example: height and width → height/width ratio for fruit shape; day/time → is_weekend flag.\n",
    "- Tip: Simple engineered features often beat complex models; iterate and validate.\n",
    "\n",
    "**Feature Selection**\n",
    "- What: Remove irrelevant or harmful features to improve generalization and speed.\n",
    "- When: Many features, noisy or redundant inputs, or overfitting risk.\n",
    "- Example: drop near‑constant columns, use mutual information, L1 regularization, or tree‑based feature importances.\n",
    "- Tip: Combine automated selection with domain checks; always evaluate on hold‑out set.\n",
    "\n",
    "\n",
    "Transform inputs so models can use them, engineer features to expose signal, and select features to reduce noise and overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f9c365",
   "metadata": {},
   "source": [
    "Whenever we are building a machine learning model that uses a supervised learning algorithm, it is important we have some way of evaluating the performance of that model. \n",
    "\n",
    "However, learning the parameters of the prediction function and testing it on the same data would mean a model would just be repeating the labels of the samples it has just seen, deriving a perfect score, but failing to predict anything useful on as-of-yet unseen data. \n",
    "\n",
    "This situation is known as overfitting, and to avoid it the most common practice is to hold out part of the available data as a test set.\n",
    "\n",
    "The most simple way to do this is by using a technique called the train-test split on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa743f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ad43d1",
   "metadata": {},
   "source": [
    "In this case, we want to have a train_size of 80% and a test size of 20%.\n",
    "\n",
    "We also want to shuffle our data before splitting as, without this, we risk creating batch data not representative of the overall dataset.\n",
    "\n",
    "Finally, we use the random_state parameter to control the shuffling applied to the data before the split to ensure a reproducible output across multiple calls of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd7db039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (455, 2)\n",
      "x_test shape: (114, 2)\n",
      "y_train shape: (455,)\n",
      "y_test shape: (114,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(standardized_data, labels, test_size = 0.20, shuffle=True, random_state = 42)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2106d0",
   "metadata": {},
   "source": [
    "### **3. Model Building**\n",
    "\n",
    "Once we have pre-processed the data into an acceptable format, we then build an ML model. This iswhere most of the “real ML” takes place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7a4a81",
   "metadata": {},
   "source": [
    "#### **3.1. Algorithm Selection**\n",
    "\n",
    "These algorithms include:\n",
    "- Linear and Polynomial models — simple, interpretable models; polynomial features capture non‑linear relationships.\n",
    "- Logit models — logistic regression for probabilistic binary/multiclass classification.\n",
    "- Maximum margin models — SVMs that maximize class separation; effective in high dimensions.\n",
    "- Tree-based models — decision trees that handle non‑linearity and interactions with easy interpretation.\n",
    "- Ensemble Models — bagging/boosting (Random Forest, Gradient Boosting) for improved accuracy and robustness.\n",
    "- Bayesian models — probabilistic approaches (e.g., Naive Bayes) that incorporate priors and quantify uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ebed12",
   "metadata": {},
   "source": [
    "Here, for most tabular problem, tree-based models have shown great performance then most ML models and even many DL models. However, for the sake of the course, we will *start with the simplest option.* \n",
    "\n",
    "**The logistic regression classifier.**\n",
    "\n",
    "<!-- A logistic regression classifier is a supervised machine learning model that predicts the probability of a categorical outcome (like yes/no, spam/not spam) by fitting an S-shaped curve (sigmoid function) to the data, outputting values between 0 and 1, which are then thresholded to assign a class label. \n",
    "\n",
    "![Logistic Regression Classifier](https://zd-brightspot.s3.us-east-1.amazonaws.com/wp-content/uploads/2022/04/11040521/46-4-e1715636469361.png) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab63ac7c",
   "metadata": {},
   "source": [
    "<!-- Sigmoid function formula: \n",
    "\n",
    "![sigmoid](https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/sigmoid-activation-function-1_0.png) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c45e44",
   "metadata": {},
   "source": [
    "#### **3.2. Loss function Selection** \n",
    "\n",
    "After selecting a specific algorithm, we need to decide on its loss function: the method which\n",
    "the algorithm would use to learn from the data. (Yes, there are different ways of learning for an\n",
    "algorithm) \n",
    "\n",
    "For example, a linear regression typically uses the famous least squares error loss function. \n",
    "\n",
    "The selection of the learning algorithm and the selection of the loss function are, of course, coupled: for example, you cannot use a software\n",
    "package that implements a linear regression algorithm with a misclassification or regression loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c3de9f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "## loss function selection\n",
    "\n",
    "# here we do not to select a loss function manually because sklearn's LogisticRegression class uses the log-loss function by default for binary classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07599218",
   "metadata": {},
   "source": [
    "Therefore, by default loss function for Logistic Regression is `log-loss` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d91e6c5",
   "metadata": {},
   "source": [
    "#### **3.3. Model Learning**\n",
    "\n",
    "Once we have selected the learning algorithm and its loss function, we need to train the model. \n",
    "\n",
    "It is simply a mathematical optimization problem. \n",
    "\n",
    "At a high level, learning amounts to finding the set of parameters that minimizes\n",
    "the loss function on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d19d4a",
   "metadata": {},
   "source": [
    "In this case, we then train the model with the following line of code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c32b44cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=42).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bcde8b",
   "metadata": {},
   "source": [
    "#### **3.4. Model Evaluation**\n",
    "\n",
    "Evaluating performance: To assess how well a model performs, we need to test it on unseen data (test data), not just the training data, to avoid overfitting.\n",
    "\n",
    "Training vs. test data: Ideally, we would have separate training and test datasets, but in practice, we often split a single large dataset into two parts for training and testing.\n",
    "\n",
    "Advanced methods: In some cases, more advanced techniques like cross-validation are used, where the data is split into multiple parts and the model is tested on different combinations to get a better understanding of its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "785ceb23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9298245614035088"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model.score(x_test, y_test) \n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d18a871",
   "metadata": {},
   "source": [
    "Now, accuracy is not the only metric that can be used to evaluate model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68cfc49",
   "metadata": {},
   "source": [
    "For this, we must first use our model to predict the labels of the test feature data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e77df43",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99493f4",
   "metadata": {},
   "source": [
    "We now want to use these predicted labels to derive a confusion matrix. A confusion matrix displays the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n",
    "\n",
    "`TPs` are test results that correctly indicate the presence of a condition or characteristic.\n",
    "\n",
    "`TNs` are test results that correctly indicate the absence of a condition or characteristic.\n",
    "\n",
    "`FPs` are test results that incorrectly indicate the presence of a condition or characteristic.\n",
    "\n",
    "`FNs` are test results that incorrectly indicate the absence of a condition or characteristic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5af77bc",
   "metadata": {},
   "source": [
    "Now let's import the confusion matrix helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "38d7cc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[38  5]\n",
      " [ 3 68]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, pred_labels)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8b079d",
   "metadata": {},
   "source": [
    "From this confusion matrix, we can then derive further metrics, including precision, recall, and the F1-score.\n",
    "\n",
    "`Precision` is the ratio of correctly classified positive instances to the total predicted positive classifications.\n",
    "\n",
    "`Recall` or sensitivity is the ratio of correctly classified positive instances to the total positive instances.\n",
    "\n",
    "Precision helps us understand how useful results are; however, recall helps us understand how complete the results are.\n",
    "\n",
    "The F1-score balances the two previous scores, being the harmonic mean of precision and recall.\n",
    "\n",
    "Note: accuracy is the ratio of correctly classified instances to the total instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e854204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e288dcd",
   "metadata": {},
   "source": [
    "For precision, recall, and the F1-score, you should report the average and standard deviation of these scores. This is because the precision, recall, and F1-score is provided for each class in the dataset.\n",
    "\n",
    "However, particularly when working with multiclass data, it is important to understand the performance of the model for each class. For this, we print a classification report, which tells you the precision, recall, and F1-score for each class and support (weighted by number of instances in each class in comparison to total number of instances for each class, respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1f81a505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "f1:\t\t 0.9444444444444444\n",
      "recall\t\t 0.9577464788732394\n",
      "precision\t 0.9315068493150684\n",
      "\n",
      "f1_avg:\t\t 0.9246031746031746\n",
      "recall_avg\t 0.9207337045528987\n",
      "precision_avg\t 0.9291680588038758\n",
      "\n",
      "f1_sd:\t\t 0.019841269841269826\n",
      "recall_sd\t 0.03701277432034061\n",
      "precision_sd\t 0.0023387905111927343\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.90        43\n",
      "           1       0.93      0.96      0.94        71\n",
      "\n",
      "    accuracy                           0.93       114\n",
      "   macro avg       0.93      0.92      0.92       114\n",
      "weighted avg       0.93      0.93      0.93       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "f1 = f1_score(y_test, pred_labels)\n",
    "recall = recall_score(y_test, pred_labels)\n",
    "precision = precision_score(y_test, pred_labels)\n",
    "\n",
    "f1_avg = mean(f1_score(y_test, pred_labels, average=None))\n",
    "recall_avg = mean(recall_score(y_test, pred_labels, average=None))\n",
    "precision_avg = mean(precision_score(y_test, pred_labels, average=None))\n",
    "\n",
    "f1_sd = std(f1_score(y_test, pred_labels, average=None))\n",
    "recall_sd = std(recall_score(y_test, pred_labels, average=None))\n",
    "precision_sd = std(precision_score(y_test, pred_labels, average=None))\n",
    "\n",
    "print('\\nf1:\\t\\t',f1)\n",
    "print('recall\\t\\t',recall)\n",
    "print('precision\\t',precision)\n",
    "\n",
    "print('\\nf1_avg:\\t\\t',f1_avg)\n",
    "print('recall_avg\\t',recall_avg)\n",
    "print('precision_avg\\t',precision_avg)\n",
    "\n",
    "print('\\nf1_sd:\\t\\t',f1_sd)\n",
    "print('recall_sd\\t',recall_sd)\n",
    "print('precision_sd\\t',precision_sd)\n",
    "\n",
    "print('\\n',classification_report(y_test, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652c1f1a",
   "metadata": {},
   "source": [
    "#### **3.5. Hyper-paramter Tuning**\n",
    "\n",
    "Hyper-paramter: A hyperparameter is a setting or configuration used to control the learning process of a machine learning model. Unlike model parameters (like weights in a neural network, which are learned from the data), hyperparameters are set before training the model and determine how the model behaves during training. Example: \n",
    "- learning rate\n",
    "- regularization strength\n",
    "- batch size etc. \n",
    "\n",
    "Overfitting and underfitting: A common challenge in machine learning is finding the right balance between `overfitting` (where the model is too complex and fits the training data too well) and `underfitting` (where the model is too simple and doesn't capture enough of the data's patterns). This balance is achieved through hyperparameter tuning.\n",
    "\n",
    "Hyperparameter tuning: The best hyperparameters (settings that control the model's behavior) aren't usually known upfront. We often have to try many different combinations to find the ones that work best. For example, in the fruit classification task, we tested different values of 𝑘.\n",
    "k in a KNN model. Too many neighbors can lead to underfitting, while too few can lead to overfitting.\n",
    "\n",
    "Algorithm-specific tuning: Each machine learning algorithm has its own set of hyperparameters, and the process of adjusting these parameters to improve the model's performance is called hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cd417a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter will be tuned in the upcoming notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f921de7",
   "metadata": {},
   "source": [
    "#### **3.6. Model Validation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc2a494",
   "metadata": {},
   "source": [
    "Now, even using a train-test split, there is still a risk of `overfitting` on the test set when tuning the parameters of a model until the estimator performs optimally. In this scenario, knowledge can leak into the model until th evaluation metrics no longer report on generalization performance.\n",
    "\n",
    "One way of solving this is hold yet another part of the available dataset out as a so-called validation set, on which an initial evaluation is done. However, the problem with partitioning the available data into three sets is that we drastically reduce the number of samples which can be used for learning the model (a particular issue for small datasets and imbalanced datasets). It also means that the results can depend on any random choice for the pair of train and validation sets.\n",
    "\n",
    "A solution to this and another way of evaluating model performance is to use `cross-validation`, which removes the need for a validation set.\n",
    "\n",
    "In the basic approach, otherwise known as k-fold cross-validation, the data is split into k folds. The model is trained using (k - 1) of the folds as training data and validation on the remaining part of the data. This is repeated for each fold. The performance measures reported by k-fold cross-validation is then the average of the values computed in the loop.\n",
    "\n",
    "![Cross Validation](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6e56ea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "model = LogisticRegression(random_state=42) # reset the model\n",
    "\n",
    "cv_pred_labels = cross_val_predict(model, standardized_data, labels, cv=10) # Here we use a 10-fold cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c5cae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:\t 0.8980667838312829\n",
      "[[173  39]\n",
      " [ 19 338]]\n",
      "\n",
      "f1:\t\t 0.9209809264305178\n",
      "recall\t\t 0.9467787114845938\n",
      "precision\t 0.896551724137931\n",
      "\n",
      "f1_avg:\t\t 0.888708284997437\n",
      "recall_avg\t 0.8814082236668253\n",
      "precision_avg\t 0.8987966954022988\n",
      "\n",
      "f1_sd:\t\t 0.03227264143308067\n",
      "recall_sd\t 0.06537048781776861\n",
      "precision_sd\t 0.00224497126436779\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.82      0.86       212\n",
      "           1       0.90      0.95      0.92       357\n",
      "\n",
      "    accuracy                           0.90       569\n",
      "   macro avg       0.90      0.88      0.89       569\n",
      "weighted avg       0.90      0.90      0.90       569\n",
      "\n",
      "0.8814082236668251\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(labels, cv_pred_labels)\n",
    "\n",
    "cm = confusion_matrix(labels, cv_pred_labels)\n",
    "\n",
    "f1 = f1_score(labels, cv_pred_labels)\n",
    "recall = recall_score(labels, cv_pred_labels)\n",
    "precision = precision_score(labels, cv_pred_labels)\n",
    "\n",
    "f1_avg = mean(f1_score(labels, cv_pred_labels, average=None))\n",
    "recall_avg = mean(recall_score(labels, cv_pred_labels, average=None))\n",
    "precision_avg = mean(precision_score(labels, cv_pred_labels, average=None))\n",
    "\n",
    "f1_sd = std(f1_score(labels, cv_pred_labels, average=None))\n",
    "recall_sd = std(recall_score(labels, cv_pred_labels, average=None))\n",
    "precision_sd = std(precision_score(labels, cv_pred_labels, average=None))\n",
    "\n",
    "print('accuracy:\\t', accuracy)\n",
    "\n",
    "print(cm)\n",
    "\n",
    "print('\\nf1:\\t\\t',f1)\n",
    "print('recall\\t\\t',recall)\n",
    "print('precision\\t',precision)\n",
    "\n",
    "print('\\nf1_avg:\\t\\t',f1_avg)\n",
    "print('recall_avg\\t',recall_avg)\n",
    "print('precision_avg\\t',precision_avg)\n",
    "\n",
    "print('\\nf1_sd:\\t\\t',f1_sd)\n",
    "print('recall_sd\\t',recall_sd)\n",
    "print('precision_sd\\t',precision_sd)\n",
    "\n",
    "print('\\n',classification_report(labels, cv_pred_labels))\n",
    "\n",
    "# print(roc_auc_score(labels, cv_pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afe1eef",
   "metadata": {},
   "source": [
    "**Iterative Development of ML pipeline**\n",
    "\n",
    "\n",
    "Building a machine learning model is an iterative process: You often test different algorithms, features, and settings multiple times to find the best combination.\n",
    "\n",
    "It’s common to switch algorithms (e.g., from a decision tree to a neural network) and experiment with different configurations to see what works best for the data.\n",
    "\n",
    "While the final prediction follows a sequential process, the model-building and tuning phase involves going back and forth, adjusting, and re-testing until the model performs optimally."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_practical_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
