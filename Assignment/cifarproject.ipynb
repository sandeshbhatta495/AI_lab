{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CIFAR-10 Classification: Comparing Simple , AlexNet, and TinyVGG\n"
      ],
      "metadata": {
        "id": "YlW4WxlMF8mK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision"
      ],
      "metadata": {
        "id": "uDkEPSHtF8mP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ],
      "metadata": {
        "id": "xDDrPtmLbQ3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Loading and Augmentation"
      ],
      "metadata": {
        "id": "0ktd2IV2F8mR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                         (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                         (0.5, 0.5, 0.5))\n",
        "])"
      ],
      "metadata": {
        "id": "pf_iLyTHF8mR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What Does Data Augmentation Do?\n",
        "\n",
        "Data augmentation generates new training samples from the original images by applying small transformations such as flipping, rotation, cropping, zooming, or shifting. Instead of feeding the model the exact same image every time, we show it slightly modified versions, which helps prevent the model from memorizing the training data and overfitting. By exposing the model to these variations, it learns general patterns rather than relying on exact pixel positions, becomes more robust to real-world variations, and improves overall generalization and test accuracy. In simple terms, data augmentation makes the dataset appear larger and more diverse, even though no new data was actually collected."
      ],
      "metadata": {
        "id": "meNa0fl1eHTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Dataset CIFAR-10\n",
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=train_transform\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=test_transform\n",
        ")"
      ],
      "metadata": {
        "id": "r5iMZP1oF8mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Loaders\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,         # No need to shuffle test data\n",
        "    num_workers=2\n",
        ")"
      ],
      "metadata": {
        "id": "kUWTlCSQdsEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# My Data info\n",
        "CLASS_NAMES = train_dataset.classes\n",
        "print('Classes:', CLASS_NAMES)\n",
        "print('\\nTraining samples:', len(train_dataset))\n",
        "print('\\nTest samples:', len(test_dataset))"
      ],
      "metadata": {
        "id": "2_5qA7tpdxg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def imshow(img_tensor):\n",
        "    img_tensor = img_tensor / 2 + 0.5\n",
        "    np_image = img_tensor.numpy()\n",
        "    plt.imshow(np.transpose(np_image, (1, 2, 0)))\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "data_iter = iter(train_loader)\n",
        "sample_images, sample_labels = next(data_iter)\n",
        "\n",
        "imshow(torchvision.utils.make_grid(sample_images[:8]))\n",
        "print(\"Labels:\", [CLASS_NAMES[label] for label in sample_labels[:8]])"
      ],
      "metadata": {
        "id": "_e-KdG3_F8mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. For Simple NN"
      ],
      "metadata": {
        "id": "-bTxwFHcF8mT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "\n",
        "        # Convert (batch, 3, 32, 32) → (batch, 3072)\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Fully Connected Block 1\n",
        "        self.fc1 = nn.Linear(32 * 32 * 3, 512)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "\n",
        "        # Fully Connected Block 2\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "\n",
        "        # Output layer (10 classes)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)                       # Flatten image\n",
        "        x = self.dropout1(self.relu1(self.fc1(x)))  # FC1 → ReLU → Dropout\n",
        "        x = self.dropout2(self.relu2(self.fc2(x)))  # FC2 → ReLU → Dropout\n",
        "        x = self.fc3(x)                           # Final logits\n",
        "        return x"
      ],
      "metadata": {
        "id": "6BKws4ZZF8mT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Neural Network, This model treats each image as a flat vector of pixels, here we use Droup out 0.3 mean 30 % of neuron is deselected at training time which prevents model from overfittings"
      ],
      "metadata": {
        "id": "W6t4tqAAgkgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Model 2  AlexNet\n",
        "class AlexNet_CIFAR10(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AlexNet_CIFAR10, self).__init__()\n",
        "\n",
        "        # Input: (batch, 3, 32, 32)\n",
        "        self.features = nn.Sequential(\n",
        "            # Fist layer\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),   # 32x32 → 16x16\n",
        "\n",
        "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),   # 16x16 → 8x8\n",
        "\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)    # 8x8 → 4x4\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256 * 4 * 4, 512),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)          # Extract spatial features\n",
        "        x = torch.flatten(x, 1)       # Flatten: (batch, 256, 4, 4) → (batch, 4096)\n",
        "        x = self.classifier(x)        # Map features to class logits\n",
        "        return x"
      ],
      "metadata": {
        "id": "q5Qj8mFZF8mU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 3: TinyVGG\n",
        "class TinyVGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TinyVGG, self).__init__()\n",
        "        # Input: (batch, 3, 32, 32)\n",
        "        self.features = nn.Sequential(\n",
        "            # Block 1\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),    # 32x32 → 16x16\n",
        "\n",
        "            # Block 2: increase depth (64 channels)\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)     # 16x16 → 8x8\n",
        "        )\n",
        "\n",
        "        # Classifier\n",
        "        # Input size: 64 channels × 8 × 8 = 4096 values\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(64 * 8 * 8, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)     # (batch, 64, 8, 8) → (batch, 4096)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "4Kk4rDOGF8mU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count Trainable Parameters\n",
        "def count_parameters(model):\n",
        "    \"\"\"Return total number of learnable parameters in the model.\"\"\"\n",
        "    total = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f'  Trainable parameters: {total:,}')\n",
        "    return total"
      ],
      "metadata": {
        "id": "Y_jppeslF8mV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def train_model(model, optimizer, scheduler, num_epochs=20, model_name='Model'):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()  # Multi-class classification loss\n",
        "\n",
        "    loss_history = []\n",
        "    accuracy_history = []\n",
        "\n",
        "    start_time = time.time()  # Track total training time\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # One epoch = model sees entire training dataset once\n",
        "        model.train()  # Enable training mode (Dropout/BatchNorm active)\n",
        "\n",
        "        running_loss = 0.0  # Sum of batch losses for this epoch\n",
        "        correct = 0         # Correct predictions counter\n",
        "        total = 0           # Total samples counter\n",
        "\n",
        "        for batch_images, batch_labels in train_loader:\n",
        "            # Each batch = subset of dataset for faster training\n",
        "\n",
        "            batch_images = batch_images.to(device)\n",
        "            batch_labels = batch_labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()  # Clear previous gradients\n",
        "\n",
        "            outputs = model(batch_images)  # Forward pass\n",
        "            loss = criterion(outputs, batch_labels)  # Compute batch loss\n",
        "\n",
        "            loss.backward()  # Backpropagation\n",
        "            optimizer.step()  # Update model weights\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Get predicted class indices\n",
        "            _, predicted_classes = torch.max(outputs, 1)\n",
        "            total += batch_labels.size(0)\n",
        "            correct += (predicted_classes == batch_labels).sum().item()\n",
        "\n",
        "        # Average loss and training accuracy for this epoch\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        train_accuracy = 100.0 * correct / total\n",
        "\n",
        "        loss_history.append(avg_loss)\n",
        "        accuracy_history.append(train_accuracy)\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()  # Adjust learning rate\n",
        "\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f'{model_name} Epoch {epoch+1}/{num_epochs} | '\n",
        "              f'Loss: {avg_loss:.4f} | '\n",
        "              f'Train Acc: {train_accuracy:.2f}% | '\n",
        "              f'Lr: {current_lr:.6f}')\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f'[{model_name}] Training finished in {total_time:.1f} seconds')\n",
        "\n",
        "    return loss_history, accuracy_history, total_time"
      ],
      "metadata": {
        "id": "UOu7gyfSF8mV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Function\n",
        "def evaluate_model(model, model_name='Model'):\n",
        "\n",
        "    model.eval()  # Set model to evaluation mode (Dropout & BatchNorm disabled)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Disable gradient calculation to save memory and speed up testing\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch_images, batch_labels in test_loader:\n",
        "            # Load data to same device (CPU/GPU)\n",
        "            batch_images = batch_images.to(device)\n",
        "            batch_labels = batch_labels.to(device)\n",
        "\n",
        "            outputs = model(batch_images)  # Forward pass only (no backprop)\n",
        "\n",
        "            # Get class with highest predicted score\n",
        "            _, predicted_classes = torch.max(outputs, dim=1)\n",
        "\n",
        "            total += batch_labels.size(0)\n",
        "            correct += (predicted_classes == batch_labels).sum().item()\n",
        "\n",
        "    # Accuracy = correct predictions / total samples\n",
        "    accuracy = 100.0 * correct / total\n",
        "\n",
        "    print(f'{model_name} Test Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "lAsDmJAfF8mW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Train All Three Models\n"
      ],
      "metadata": {
        "id": "P7mAGH4yF8mX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 1: SimpleNN with Adam"
      ],
      "metadata": {
        "id": "dIEcfjM1SoQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple NN\n",
        "print('Simple Neural Network')\n",
        "model_simplenn = SimpleNN().to(device)\n",
        "num_params_simplenn = count_parameters(model_simplenn)\n",
        "\n",
        "optimizer_adam_nn = optim.Adam(\n",
        "    model_simplenn.parameters(),\n",
        "    lr=0.001,\n",
        "    weight_decay=1e-4  # L2 regularisation\n",
        ")\n",
        "#lr shedule\n",
        "scheduler_nn = optim.lr_scheduler.StepLR(\n",
        "    optimizer_adam_nn,\n",
        "    step_size=5,\n",
        "    gamma=0.5\n",
        "    )\n",
        "\n",
        "# Train for 20 epochs\n",
        "loss_nn, acc_nn, time_nn = train_model(\n",
        "    model_simplenn, optimizer_adam_nn, scheduler_nn,\n",
        "    num_epochs=20, model_name='SimpleNN'\n",
        ")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_acc_nn = evaluate_model(model_simplenn, model_name='SimpleNN')"
      ],
      "metadata": {
        "id": "C5kzR1dmF8mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 2: AlexNet with SGD + Momentum"
      ],
      "metadata": {
        "id": "lsl3CHaMF8mX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 2 : AlexNet\n",
        "print(\"Ale\")\n",
        "model_alexnet = AlexNet_CIFAR10().to(device)\n",
        "num_params_alexnet = count_parameters(model_alexnet)\n",
        "\n",
        "optimizer_sgd_alexnet = optim.SGD(\n",
        "    model_alexnet.parameters(),\n",
        "    lr=0.01,\n",
        "    momentum=0.9,\n",
        "    weight_decay=5e-4  # L2 regularisation\n",
        ")\n",
        "# lr scheduler\n",
        "scheduler_alexnet = optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer_sgd_alexnet,\n",
        "    T_max=30,\n",
        "    eta_min=1e-4\n",
        ")\n",
        "\n",
        "# Train for 30 epochs (CNNs need more time to converge)\n",
        "loss_alexnet, acc_alexnet, time_alexnet = train_model(\n",
        "    model_alexnet, optimizer_sgd_alexnet, scheduler_alexnet,\n",
        "    num_epochs=20, model_name='AlexNet'\n",
        ")\n",
        "\n",
        "test_acc_alexnet = evaluate_model(model_alexnet, model_name='AlexNet')"
      ],
      "metadata": {
        "id": "iJgNYxsDF8mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 3: TinyVGG (opti. SGD + Momentum)"
      ],
      "metadata": {
        "id": "5CyQJf-kF8mX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 3: TinyVGG\n",
        "model_tinyvgg = TinyVGG().to(device)  # Initialize model and move to GPU/CPU\n",
        "num_params_tinyvgg = count_parameters(model_tinyvgg)  # Count total trainable parameters\n",
        "\n",
        "# Optimizer: SGD with momentum and weight decay for regularization\n",
        "optimizer_sgd_vgg = torch.optim.SGD(\n",
        "    model_tinyvgg.parameters(),\n",
        "    lr=0.05,\n",
        "    momentum=0.9,\n",
        "    weight_decay=5e-4\n",
        ")\n",
        "\n",
        "# Learning rate scheduler: Cosine Annealing\n",
        "scheduler_tinyvgg = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer_sgd_vgg,\n",
        "    T_max=30,      # Number of epochs per cycle\n",
        "    eta_min=1e-4   # Minimum learning rate\n",
        ")\n",
        "\n",
        "# Train the model and get per-epoch loss and accuracy\n",
        "loss_tinyvgg, acc_tinyvgg, time_tinyvgg = train_model(\n",
        "    model_tinyvgg,\n",
        "    optimizer_sgd_vgg,\n",
        "    scheduler_tinyvgg,\n",
        "    num_epochs=20,\n",
        "    model_name='TinyVGG'\n",
        ")\n",
        "\n",
        "# Evaluate model on test dataset\n",
        "test_acc_tinyvgg = evaluate_model(model_tinyvgg, model_name='TinyVGG')"
      ],
      "metadata": {
        "id": "KL9XQtj9F8mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Comparison and Visualisation"
      ],
      "metadata": {
        "id": "9IMgBdkeF8mY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Training Accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, len(acc_nn) + 1), acc_nn, label='SimpleNN (Adam)')\n",
        "plt.plot(range(1, len(acc_alexnet) + 1), acc_alexnet, label='AlexNet (SGD)')\n",
        "plt.plot(range(1, len(acc_tinyvgg) + 1), acc_tinyvgg, label='TinyVGG (SGD)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Training Accuracy')\n",
        "plt.title('Accuracy per Epoch')\n",
        "plt.legend()\n",
        "\n",
        "# Training Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, len(loss_nn) + 1), loss_nn, label='SimpleNN (Adam)')\n",
        "plt.plot(range(1, len(loss_alexnet) + 1), loss_alexnet, label='AlexNet (SGD)')\n",
        "plt.plot(range(1, len(loss_tinyvgg) + 1), loss_tinyvgg, label='TinyVGG (SGD)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss per Epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_curves.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sd-4OgkQF8mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nModel            Params     Train Time(s)   Test Acc (%)\")\n",
        "print(\"----------------------------------------------------------\")\n",
        "\n",
        "print(f\"SimpleNN   {num_params_simplenn:>12,}   {time_nn:>14.1f}   {test_acc_nn:>12.2f}\")\n",
        "print(f\"AlexNet    {num_params_alexnet:>12,}   {time_alexnet:>14.1f}   {test_acc_alexnet:>12.2f}\")\n",
        "print(f\"TinyVGG    {num_params_tinyvgg:>12,}   {time_tinyvgg:>14.1f}   {test_acc_tinyvgg:>12.2f}\")"
      ],
      "metadata": {
        "id": "y8rBkKX0F8mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the Model is Correctly predicting or not\n"
      ],
      "metadata": {
        "id": "aEWCZ70TuLBc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion and Conclusion :\n",
        "##From the above we find that\n",
        "##Training 3 models like Simple NN , Alexnet & Tiny VGG\n",
        "1. **Using Simple NN model :**\n",
        "It treats a image as a flat vector , which discards all spatial structure. Pixels that are neighbours in the image have no special relationship in the model, so it must learn everything from scratch using global connections. As a result, it converges slowly and plateaus at a relatively low test accuracy ~ 51% .and its have 1,707,274 parameters.\n",
        "\n",
        "2.** Using AlexNet :**\n",
        "AlexNet introduces convolutional layers, which share weights across spatial locations. This enables translation equivariance: filters detect the same patterns regardless of position. Its five convolutional layers (64 → 192 → 384 → 256 → 256 channels) allow hierarchical feature extraction—from edges and textures to object parts. Dropout (p=0.5) in fully connected layers reduces overfitting. The result is a significant accuracy boost (~77%) over the MLP, demonstrating the power of convolutions for spatially structured data.\n",
        "\n",
        "3. ** Using Tiny VGG Model :**\n",
        "TinyVGG follows the VGG philosophy of stacking small 3×3 convolutions. Two stacked 3×3 convolutions have the same receptive field as a single 5×5 convolution but use fewer parameters and introduce extra non-linearity, increasing representational power. TinyVGG achieves the highest accuracy (~82%) with the fewest parameters (1.1M), highlighting that careful architectural design can outperform larger networks.\n",
        "\n",
        "## Optimizer and Scheduler Choices\n",
        "\n",
        "SimpleNN used Adam for its adaptive per-parameter learning rates, helping navigate the flat, complex loss landscapes typical of fully connected networks.\n",
        "\n",
        "AlexNet and TinyVGG used SGD with momentum (0.9), which accelerates convergence through persistent gradient directions and often generalizes better than Adam for CNNs.\n",
        "\n",
        "Both CNNs applied CosineAnnealingLR, reducing the learning rate gradually to allow aggressive initial learning followed by fine-tuning. This improves final accuracy compared to fixed or step-based schedules."
      ],
      "metadata": {
        "id": "lu2CgV9BWqw4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7WrJNnYlWwra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusions\n",
        "\n",
        "CNNs are superior to MLPs for visual data due to spatial awareness and hierarchical feature learning.\n",
        "\n",
        "Parameter efficiency and architecture design often matter more than raw parameter count; TinyVGG is a prime example.\n",
        "\n",
        "Future improvements could include Batch Normalization, residual connections (ResNet), and transfer learning from pretrained models to push CIFAR-10 accuracy beyond 90%."
      ],
      "metadata": {
        "id": "bgvd1_lj034n"
      }
    }
  ]
}